(window.webpackJsonp=window.webpackJsonp||[]).push([[24],{281:function(e,t,a){},301:function(e,t,a){"use strict";a(281)},319:function(e,t,a){"use strict";a.r(t);a(301);var n=a(14),i=Object(n.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("ProfileSection",{attrs:{frontmatter:e.$page.frontmatter}}),e._v(" "),t("h2",{attrs:{id:"about-me"}},[e._v("About Me")]),e._v(" "),t("p",[e._v("Hi! üëã I'm Tianyang Xu, a second-year PhD student at "),t("a",{attrs:{href:"https://www.purdue.edu/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Purdue University"),t("OutboundLink")],1),e._v(".")]),e._v(" "),t("p",[e._v("My academic interests lie in the field of Natural Language Processing (NLP) and large language models (LLMs), with a focus on mitigating hallucinations, aligning models with human-centered objectives, and enhancing the overall safety and reliability of these systems. I am interested in exploring innovative approaches that ensure the effective use of LLMs in real-world applications in terms of trustworthiness and ethical use.")]),e._v(" "),t("p",[e._v("Also, I am interested in combining LLMs with other aspects of science, such as LLM + Biology and LLM + Healthcare. I am committed to utilizing AI to solve complex challenges and drive innovation at the intersection of technology and science.")]),e._v(" "),t("p",[e._v("I am willing to explore new possibilities in innovative uses of LLMs. Please don't hesitate to contact me for potential collaborations and opportunities! ‚ò∫Ô∏è")]),e._v(" "),t("h2",{attrs:{id:"education"}},[e._v("Education")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Purdue University")])]),e._v(" "),t("p",[e._v("Ph.D. Elmore Family School of Electrical and Computer Engineering")]),e._v(" "),t("ul",[t("li",[e._v("2023/8 -- 2028/5 (Expected)")]),e._v(" "),t("li",[e._v("Adviser: "),t("a",{attrs:{href:"https://engineering.purdue.edu/~jinggao/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Prof. Jing Gao"),t("OutboundLink")],1)])])])]),e._v(" "),t("br"),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Wuhan University")])]),e._v(" "),t("p",[e._v("B.Eng. School of Computer Science")]),e._v(" "),t("ul",[t("li",[e._v("2019/9 -- 2023/5")]),e._v(" "),t("li",[e._v("GPA 3.95/4.0, Rank 1/30")])])])]),e._v(" "),t("h2",{attrs:{id:"select-publications"}},[e._v("Select Publications")]),e._v(" "),t("p",[e._v("Note: {} encloses co-first authors. For full publication list, see "),t("RouterLink",{attrs:{to:"/publications/"}},[e._v("here")]),e._v(".")],1),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales")])]),e._v(" "),t("p",[t("strong",[e._v("Tianyang Xu")]),e._v(", Shujin Wu, Shizhe Diao, Xiaoze Liu, Xingyao Wang, Yangyi Chen, Jing Gao<")]),e._v(" "),t("p",[e._v("Large language models (LLMs) often generate inaccurate or fabricated information and generally fail to indicate their confidence, which limits their broader applications. In this work, we present the advanced SaySelf, a training framework that teaches LLMs to express more accurate fine-grained confidence estimates. In addition, beyond the confidence scores, SaySelf initiates the process of directing LLMs to produce self-reflective rationales that clearly identify gaps in their parametric knowledge and explain their uncertainty.")]),e._v(" "),t("p",[e._v("["),t("a",{attrs:{href:"https://arxiv.org/abs/2405.20974/",target:"_blank",rel:"noopener noreferrer"}},[e._v("EMNLP 2024"),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://github.com/xu1868/SaySelf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),t("OutboundLink")],1),e._v("]")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models Memories")])]),e._v(" "),t("p",[e._v("{Shizhe Diao, "),t("strong",[e._v("Tianyang Xu")]),e._v("}, Ruijia Xu, Jiawei Wang, Tong Zhang")]),e._v(" "),t("p",[e._v("Although continued pre-training on a large domain-specific corpus is effective, it is costly to tune all the parameters on the domain. In this paper, we investigate whether we can adapt PLMs both effectively and efficiently by only tuning a few parameters. Our proposed Mixture-of-Domain-Adapters (MixDA) employs a two-stage adapter-tuning strategy that leverages both unlabeled data and labeled data to help the domain adaptation: i) domain-specific adapter on unlabeled data; followed by ii) the task-specific adapter on labeled data. MixDA can be seamlessly plugged into the pretraining-finetuning paradigm.")]),e._v(" "),t("p",[e._v("["),t("a",{attrs:{href:"https://arxiv.org/abs/2306.05406",target:"_blank",rel:"noopener noreferrer"}},[e._v("ACL 2023"),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://github.com/xu1868/Mixture-of-Domain-Adapters",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),t("OutboundLink")],1),e._v("]")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("SHIELD: Evaluation and Defense Strategies for Copyright Compliance in LLM Text Generation")])]),e._v(" "),t("p",[e._v("Xiaoze Liu, Ting Sun, "),t("strong",[e._v("Tianyang Xu")]),e._v(", Feijie Wu, Cunxiang Wang, Xiaoqian Wang, Jing Gao")]),e._v(" "),t("p",[e._v("Large Language Models (LLMs) have transformed machine learning but raised significant legal concerns due to their potential to produce text that infringes on copyrights, resulting in several high-profile lawsuits. To tackle these challenges, we introduce a curated dataset to evaluate methods, test attack strategies, and propose lightweight, real-time defense to prevent the generation of copyrighted text, ensuring the safe and lawful use of LLMs.")]),e._v(" "),t("p",[e._v("["),t("a",{attrs:{href:"https://arxiv.org/abs/2406.12975",target:"_blank",rel:"noopener noreferrer"}},[e._v("EMNLP 2024"),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://github.com/xz-liu/SHIELD",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),t("OutboundLink")],1),e._v("]")])])]),e._v(" "),t("h2",{attrs:{id:"academic-services"}},[e._v("Academic Services")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Reviewer")])]),e._v(" "),t("p",[e._v("Served as a Conference External Reviewer for PAKDD 2023.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Teaching Assistant")])]),e._v(" "),t("p",[e._v("Served as a teaching assistant for various Purdue courses, including ECE 56200 (Data Management) and ECE 36800 (Data Structures).")])])])],1)}),[],!1,null,null,null);t.default=i.exports}}]);